{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code is using the RandomForest classifier on the Adult data set. The RandomForest is a tree-like structure.RandomForest is\n",
    "based on the concept of ‘Ensemble’ or group. In RandomForest multiple decision trees are generated (based on n_estimator values)\n",
    "This group of weak models is combined to form a powerful model and it unifies their result to obtain a good prediction.\n",
    "The approach of a voting mechanism from the aggregated prediction of all the trees gives a better chance of good and fairer\n",
    "predication.Multiple decision trees are made using either gini index or entropy\n",
    "In scikit, Decision Tree only works on numerical data. Therefore all categorical data are OneHotEncoded to integer format.\n",
    "The categorical data is transformed using the scikit library OneHotEncoder into O's and 1's.In OneHotEncoder every unique value\n",
    "of the column is transformed to feature. The row values of the original data set are now transformed into 0 and 1 values.\n",
    "The row in the new data frame as values for that column as 1 if in the original data frame the row had that is value else will \n",
    "have value 0. \n",
    "Thus new data frame will have as many columns as many unique column values in the original data set.\n",
    "\n",
    "Eg:-Let say a data frame has 2 columns. After OneHotEncoder is applied on the categorical column(Color), it will be transformed\n",
    "as shown below into a new data frame.\n",
    "    Color   Price                     \n",
    "    Red\t    2000\n",
    "    Green   5000\n",
    "    Yellow  9000\n",
    "    \n",
    "Color_Red    Color_Green   Color_Yellow   Price\n",
    "    1           0               0        2000\n",
    "    0           1               0        5000\n",
    "    0           0               1        9000\n",
    "    \n",
    "    \n",
    "The code implements both train-test split.\n",
    "\n",
    "The code also implements the scikit GridSearchCV library. This library takes multiple classifier objects. For each of the classifiers,\n",
    "you can pass multiple parameters with a range of values. The library will execute each of the classifiers object with \n",
    "permutation and combination of their respective parameter, create and store different models based on this. In the end, \n",
    "the model which gives the maximum accuracy score will be displayed along with the optimal parameter and values.\n",
    "\n",
    "\n",
    "Details of the algorithm are given below:-\n",
    "\n",
    "1. Data is read into a pandas data frame. The columns having '?' are replaced with the mode of those columns. The data is then \n",
    "split into X(input) and Y(output/class).\n",
    "\n",
    "2. The X data are separated respectively into a continuous and categorical data frame.\n",
    "\n",
    "3. The categorical data is transformed into integer format using the OneHotEncoder library. The categorical X data \n",
    "   is fit and transformed on the object of OneHotEncoder. The output is an array where every unique value of the \n",
    "   column is transformed into a feature. The original rows in the array are now replaced with 0's and 1's as values.\n",
    "   \n",
    "4. Concatenate transformed categorical and continuous data. Train-test split this data into training and test data.\n",
    "\n",
    "5. Train-test split the data obtained from point 4\n",
    "\n",
    "6. Create an object of RandomForest with all default parameters except criterion ='entropy'.Fit the training and Y data to build a\n",
    "model.\n",
    "\n",
    "7. Predict test data over the object of RandomForest ie model created\n",
    "in point 6.\n",
    "\n",
    "8. Find the accuracy score of the predicted data and actual Y data.\n",
    "\n",
    "\n",
    "\n",
    "RandomForest and K-nearest classifier with GridSearch --\n",
    "\n",
    "1. The points 1-4 from the above will remain the same.\n",
    "\n",
    "2. Create a Pipeline object listing classifier objects.\n",
    "\n",
    "3. Create a param_grid dictionary with parameters for each of the classifiers and a range of values.\n",
    "\n",
    "3. Create object of GridSearchCV with pipeline object,param_grid dictionary and cv=3 (cross validation).\n",
    "\n",
    "4. Fit the training data on this object.\n",
    "\n",
    "5. The attributes of gridsearchcv are printed to give the best classifier with optimal parameter and values along with accuracy\n",
    "score.\n",
    "\n",
    "In the end, you will an accuracy score to compare train-test and cross-validation approaches.\n",
    "Also, compare and choose the best classifier for the Adult data set.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    '''\n",
    "    This function is to read data using Pandas read_csv function and convert into dataframe.\n",
    "    The '?' values of the columns are replaced with the mode value of those columns.\n",
    "    \n",
    "    Return :- \n",
    "    data :- Dataframe of adult data set\n",
    "    \n",
    "    '''\n",
    "    data=pd.read_csv('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data',names=['Age','Workclass','FNLWGT','Education','Education-Num','Marital Status','Occupation','Relationship','Race','Sex','Capital Gain','Caplital Loss','Hrs-Per-Week','Native-Country','Sal'])\n",
    "    \n",
    "      \n",
    "    for col in data.columns:\n",
    "\n",
    "        data[col].replace(' ?',data[col].mode()[0],inplace=True)\n",
    "        \n",
    "        data['Native-Country'].replace(' Trinadad&Tobago' ,'TrinadadTobago',inplace=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \n",
    "    '''\n",
    "    The function is to split data first into the Input (X) and Output (Y) dataset\n",
    "    \n",
    "    Argument :-\n",
    "    df :- Adult data set\n",
    "    \n",
    "    Return :-\n",
    "    x :- X data\n",
    "    y:- Y data\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    x=df.iloc[:,:-1]\n",
    "    y=df.iloc[:,-1]\n",
    "    \n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencode(y_data):\n",
    "    \n",
    "    '''\n",
    "    The function is to convert the Y data into labelencode. We are doing this for graphical display of Decision Tree.\n",
    "    Data format may not be right,so labelencoding the data.\n",
    "    \n",
    "    Argument :- \n",
    "    y_data :- Y data\n",
    "    \n",
    "    Return :- \n",
    "    y_data :- Labelencoded Y data\n",
    "    le    :- Object of LabelEncoder\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    le=LabelEncoder()\n",
    "    \n",
    "    y_data=le.fit_transform(y_data)\n",
    "    \n",
    "    return y_data,le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_cont_data_split(xdata):\n",
    "    \n",
    "    '''\n",
    "    The function is to split the X data into categorical and continuous data.\n",
    "            \n",
    "    Argument :- \n",
    "    xdata :- X data\n",
    "    \n",
    "    Return :- \n",
    "    x_cat_data :- Categorical data frame.\n",
    "    x_cont_data :- Continuous data frame\n",
    "    \n",
    "      \n",
    "    '''\n",
    "        \n",
    "    #split xdata data into categorical data\n",
    "\n",
    "    x_cat_data=xdata[['Workclass','Education','Marital Status','Occupation','Relationship','Race','Sex','Native-Country']]\n",
    "        \n",
    "    #split xdata into continuous data\n",
    "\n",
    "    x_cont_data=xdata[['Age','FNLWGT','Education-Num','Capital Gain','Caplital Loss','Hrs-Per-Week']]\n",
    "        \n",
    "    \n",
    "    return x_cat_data,x_cont_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(datatofit):\n",
    "    \n",
    "    '''\n",
    "   The function is to convert the categorical data into integer format using the OneHotEncoder library. The categorical X data \n",
    "   is fit and transformed on the object of OneHotEncoder. The output is an array where every unique value of the \n",
    "   columns are transformed into a feature. The original rows in the array are now replaced with 0's and 1's as values.\n",
    "    \n",
    "    Argument :- \n",
    "    datatofit :-X-catogorical data\n",
    "    \n",
    "    Return :- \n",
    "    x_data_oe :-Onehot encoded catogorical data.\n",
    "    oe        :- Object of OneHotEncoder\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    oe=OneHotEncoder()\n",
    "    x_data_oe=oe.fit_transform(datatofit).toarray()\n",
    "    \n",
    "    return x_data_oe,oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_cat_cont(xcat_ohe,xcont_data):\n",
    "    \n",
    "    '''\n",
    "    The function is to concatenate transformed categorical and continuous data. \n",
    "    \n",
    "    \n",
    "    Argument :- \n",
    "    xcat_ohe   :- onehotencoder data \n",
    "    xcont_data :-Continuous data.\n",
    "    \n",
    "    Return :- \n",
    "    concat_df :-Dataframe with both catogorical and continuous data.\n",
    "             \n",
    "    '''\n",
    "    xcat_ohe=pd.DataFrame(xcat_ohe)\n",
    "    concat_df=pd.concat([xcont_data,xcat_ohe],axis=1)\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(concat_cat_cont_df,ydata_lb):\n",
    "    \n",
    "    '''\n",
    "    The function is to train-test split the dataframe with transformed categorical data and continuous data.\n",
    "    \n",
    "    Argument :- \n",
    "    concat_cat_cont_df  :- Concatenated cat and cont data,\n",
    "    ydata_lb               :- LabelEncoded Y data.\n",
    "    \n",
    "    Return :- \n",
    "    x_train:-training data\n",
    "    x_test :- Validation data\n",
    "    y_train :- Label data of training data\n",
    "    y_test :- label data of validation data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    x_train,x_test,y_train,y_test=train_test_split(concat_cat_cont_df,ydata_lb,test_size=0.3,random_state=0)\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(xtrain,ydata):\n",
    "    '''\n",
    "    Create an object of DT and fit the training X and Y data.\n",
    "    \n",
    "    Argument :- xtrain and ydata.\n",
    "    \n",
    "    Return :- object of DT\n",
    "    \n",
    "    '''\n",
    "    clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=30, min_samples_split=40,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, \n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "    \n",
    "    \n",
    "    clf.fit(xtrain,ydata)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(rf_obj,x_test):\n",
    "    \n",
    "    '''\n",
    "    The function is to run the test data over the model to get the prediction of every row.\n",
    "    \n",
    "    Argument :- \n",
    "    rf_obj :- Decision Tree object \n",
    "    x_test  :- test data.\n",
    "    \n",
    "    Return :- \n",
    "    ypred :- Array of predicted class for every row.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    y_pred=rf_obj.predict(x_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acccuracy_score(ypred,y_test):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the accuracy score of predicted data and actual Y data of test data set.\n",
    "    \n",
    "    Argument :- \n",
    "    y_test :- Test data\n",
    "    ypred :-Predicted arrray of class \n",
    "            \n",
    "    '''\n",
    "    \n",
    "\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_dt():\n",
    "    #knn_grid = {'n_neighbors': list(range(3,10,2))}\n",
    "    \n",
    "    parameters={'n_estimators':[10,100],'max_depth': range(1,10,1),'min_samples_leaf': range(1,15,5),'min_samples_split':range(2,50,2),'min_impurity_decrease':np.arange(0.0,0.1,0.01)}\n",
    "    #parameters={'min_samples_split':range(4,50,2)}\n",
    "    clf_tree=RandomForestClassifier(n_estimators=100,criterion=\"entropy\")\n",
    "    clf=GridSearchCV(clf_tree,parameters,cv=3)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    print(clf.best_estimator_)\n",
    "    print(clf.best_score_)\n",
    "    best_obj=clf.best_estimator_\n",
    "    \n",
    "    return best_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_dt():\n",
    "    \n",
    "    '''\n",
    "    The function is to run multiple classifiers using GridSearchCV and the find the best classifier.The library takes multiple\n",
    "    classifier objects. Create a param_grid dictionary with parameters for each of the classifiers and a range of values. \n",
    "    Create a Pipeline object listing classifier objects.Create a object of GridSearchCV with pipeline object,param_grid \n",
    "    dictionary and cv=3 (cross-validation).Fit the training data on this object. The attributes of GridSearchCV are printed to\n",
    "    give the best classifier with optimal parameters and values along with an accuracy score.\n",
    "    \n",
    "    '''\n",
    "    k_range = list(range(3,9,2))\n",
    "    m_depth=list(range(10,12))\n",
    "    m_split=list(range(30,35))\n",
    "    est_range = list(range(50,100,10))\n",
    "    \n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "\n",
    "    param_grid = [{\n",
    "        'classifier':[KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors':k_range,\n",
    "        },\n",
    "        {\n",
    "        'classifier':[RandomForestClassifier()],\n",
    "        'classifier__max_depth': m_depth,\n",
    "        'classifier__n_estimators':est_range,\n",
    "        'classifier__min_samples_split':m_split,\n",
    "        'classifier__criterion':['entropy','gini']\n",
    "       }] \n",
    "\n",
    "\n",
    "    clf=GridSearchCV(pipe,param_grid,cv=3)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    print(clf.best_estimator_)\n",
    "    print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data using pandas and convert in data frame\n",
    "df=read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into X and Y data.\n",
    "xdata,ydata=preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncode Y data for graphically displaying the Decision Tree.\n",
    "ydata_lb,le_obj=labelencode(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the X data into categorical and continuous data.\n",
    "xcat_data,xcont_data=cat_cont_data_split(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the categorical data into interger format using OneHotEncoder.\n",
    "xcat_ohe,oe_obj=onehotencode(xcat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate transformed categorical data with continuous data.\n",
    "concat_cat_cont_df=concat_cat_cont(xcat_ohe,xcont_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split the concatenated data.\n",
    "xtrain,xtest,ytrain,ytest=train_test(concat_cat_cont_df,ydata_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create object of RandomForest classifier.\n",
    "rf_obj=random_forest(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the test data over the model to predict the data.\n",
    "y_pred=pred_test(rf_obj,xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8554611526256526\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy score of predicted and actual Y data.\n",
    "acccuracy_score(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=11, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=32,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "0.8578448578448579\n"
     ]
    }
   ],
   "source": [
    "best_est_obj=grid_search_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
