{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code is using the Decision Tree classifier on the Adult data set. The Decision Tree is a tree-like structure. The topmost\n",
    "is the root node. Below the root nodes are the internal nodes/branches which are decision rules for further splitting the \n",
    "internal  nodes. In the end, we have the leaf node, which corresponds to a pure class. The decision tree uses either Entropy\n",
    "or gini along with information gain as a deciding factor to create a root node and branches, leaf node below it.\n",
    "In scikit, Decision tree only works on numerical data. Therefore all categorical data are OneHotEncoded to integer format.\n",
    "The categorical data is transformed using the scikit library OneHotEncoder into O's and 1's.In OneHotEncoder every unique value\n",
    "of the column is transformed to feature. The row values of the original data set are now transformed into 0 and 1 values.\n",
    "The row in the new data frame as values for that column as 1 if in the original data frame the row had that is value else will \n",
    "have value 0. \n",
    "Thus new data frame will have as many columns as many unique column values in the original data set.\n",
    "\n",
    "Eg:-Let say a data frame has 2 columns. After OneHotEncoder is applied on the categorical column(Color), it will be transformed\n",
    "as shown below into a new data frame.\n",
    "    Color   Price                     \n",
    "    Red\t    2000\n",
    "    Green   5000\n",
    "    Yellow  9000\n",
    "    \n",
    "Color_Red    Color_Green   Color_Yellow   Price\n",
    "    1           0               0        2000\n",
    "    0           1               0        5000\n",
    "    0           0               1        9000\n",
    "    \n",
    "    \n",
    "The code implements both train-test split and cross-validation to see if there is any change in the model and accuracy score.\n",
    "\n",
    "The code also implements the scikit GridSearchCV library. This library takes multiple classifier objects. For each of the classifiers,\n",
    "you can pass multiple parameters with a range of values. The library will execute each of the classifiers object with \n",
    "permutation and combination of their respective parameter, create and store different models based on this. In the end, \n",
    "the model which gives the maximum accuracy score will be displayed along with the optimal parameter and values.\n",
    "\n",
    "\n",
    "Details of the algorithm are given below:-\n",
    "\n",
    "1. Data is read into a pandas data frame. The columns having '?' are replaced with the mode of those columns. The data is then \n",
    "split into X(input) and Y(output/class).\n",
    "\n",
    "2. The X data are separated respectively into a continuous and categorical data frame.\n",
    "\n",
    "3. The categorical data is transformed into integer format using the OneHotEncoder library. The categorical X data \n",
    "   is fit and transformed on the object of OneHotEncoder. The output is an array where every unique value of the \n",
    "   column is transformed into a feature. The original rows in the array are now replaced with 0's and 1's as values.\n",
    "   \n",
    "4. Concatenate transformed categorical and continuous data. Train-test split this data into training and test data.\n",
    "\n",
    "5. Train-test split the data obtained from point 4\n",
    "\n",
    "6. Create an object of Decision Tree with all default parameters except criterion ='entropy'.Fit the training and Y data to build a\n",
    "model.\n",
    "\n",
    "7. Predict test data over the object of decision tree ie model created\n",
    "in point 6.\n",
    "\n",
    "8. Find the accuracy score of the predicted data and actual Y data.\n",
    "\n",
    "Decision Tree with cross_val_score --\n",
    "\n",
    "The decision tree is executed with cross-validation using the scikit method cross_val_score.\n",
    "\n",
    "Steps for the execution of the code:-\n",
    "\n",
    "1. The point 1-4 from above will remain the same.\n",
    "2. Execute the cross_val_score with decision tree object,training data,Y data,cv=5,scoring=accuracy parameter and values.\n",
    "The method will run with 5 folds. It will return the average accuracy score of the folds.\n",
    "\n",
    "\n",
    "Decision Tree and K-nearest classifier with GridSearch --\n",
    "\n",
    "1. The points 1-4 from the above will remain the same.\n",
    "\n",
    "2. Create a Pipeline object listing classifier objects.\n",
    "\n",
    "3. Create a param_grid dictionary with parameters for each of the classifiers and a range of values.\n",
    "\n",
    "3. Create object of GridSearchCV with pipeline object,param_grid dictionary and cv=3 (cross validation).\n",
    "\n",
    "4. Fit the training data on this object.\n",
    "\n",
    "5. The attributes of gridsearchcv are printed to give the best classifier with optimal parameter and values along with accuracy\n",
    "score.\n",
    "\n",
    "In the end, you will an accuracy score to compare train-test and cross-validation approaches.\n",
    "Also, compare and choose the best classifier for the Adult data set.\n",
    "\n",
    "The code also has a method to graphically represent the Decision tree and also save the image as a pdf file.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pydotplus\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import tree\n",
    "import os \n",
    "import graphviz\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    '''\n",
    "    This function is to read data using Pandas read_csv function and convert into dataframe.\n",
    "    The '?' values of the columns are replaced with the mode value of those columns.\n",
    "    \n",
    "    Return :- \n",
    "    data :- Dataframe of adult data set\n",
    "    \n",
    "    '''\n",
    "    data=pd.read_csv('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data',names=['Age','Workclass','FNLWGT','Education','Education-Num','Marital Status','Occupation','Relationship','Race','Sex','Capital Gain','Caplital Loss','Hrs-Per-Week','Native-Country','Sal'])\n",
    "    \n",
    "      \n",
    "    for col in data.columns:\n",
    "\n",
    "        data[col].replace(' ?',data[col].mode()[0],inplace=True)\n",
    "        \n",
    "        data['Native-Country'].replace(' Trinadad&Tobago' ,'TrinadadTobago',inplace=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \n",
    "    '''\n",
    "    The function is to split data first into the Input (X) and Output (Y) dataset\n",
    "    \n",
    "    Argument :-\n",
    "    df :- Adult data set\n",
    "    \n",
    "    Return :-\n",
    "    x :- X data\n",
    "    y:- Y data\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    x=df.iloc[:,:-1]\n",
    "    y=df.iloc[:,-1]\n",
    "    \n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencode(y_data):\n",
    "    \n",
    "    '''\n",
    "    The function is to convert the Y data into labelencode. We are doing this for graphical display of Decision Tree.\n",
    "    Data format may not be right,so labelencoding the data.\n",
    "    \n",
    "    Argument :- \n",
    "    y_data :- Y data\n",
    "    \n",
    "    Return :- \n",
    "    y_data :- Labelencoded Y data\n",
    "    le    :- Object of LabelEncoder\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    le=LabelEncoder()\n",
    "    \n",
    "    y_data=le.fit_transform(y_data)\n",
    "    \n",
    "    return y_data,le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_cont_data_split(xdata):\n",
    "    \n",
    "    '''\n",
    "    The function is to split the X data into categorical and continuous data.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Argument :- \n",
    "    xdata :- X data\n",
    "    \n",
    "    Return :- \n",
    "    x_cat_data :- Categorical data frame.\n",
    "    x_cont_data :- Continuous data frame\n",
    "    \n",
    "      \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #split xdata data into categorical data\n",
    "\n",
    "    x_cat_data=xdata[['Workclass','Education','Marital Status','Occupation','Relationship','Race','Sex','Native-Country']]\n",
    "       \n",
    "    #split xdata into continuous data\n",
    "\n",
    "    x_cont_data=xdata[['Age','FNLWGT','Education-Num','Capital Gain','Caplital Loss','Hrs-Per-Week']]\n",
    "    \n",
    "    \n",
    "    return x_cat_data,x_cont_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(datatofit):\n",
    "    \n",
    "    '''\n",
    "   The function is to convert the categorical data into integer format using the OneHotEncoder library. The categorical X data \n",
    "   is fit and transformed on the object of OneHotEncoder. The output is an array where every unique value of the \n",
    "   columns are transformed into a feature. The original rows in the array are now replaced with 0's and 1's as values.\n",
    "    \n",
    "    Argument :- \n",
    "    datatofit :-X-catogorical data\n",
    "    \n",
    "    Return :- \n",
    "    x_data_oe :-Onehot encoded catogorical data.\n",
    "    oe        :- Object of OneHotEncoder\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    oe=OneHotEncoder()\n",
    "    x_data_oe=oe.fit_transform(datatofit).toarray()\n",
    "    \n",
    "    return x_data_oe,oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_cat_cont(xcat_ohe,xcont_data):\n",
    "    \n",
    "    '''\n",
    "    The function is to concatenate transformed categorical and continuous data. \n",
    "    \n",
    "    \n",
    "    Argument :- \n",
    "    xcat_ohe   :- onehotencoder data \n",
    "    xcont_data :-Continuous data.\n",
    "    \n",
    "    Return :- \n",
    "    concat_df :-Dataframe with both catogorical and continuous data.\n",
    "             \n",
    "    '''\n",
    "    xcat_ohe=pd.DataFrame(xcat_ohe)\n",
    "    concat_df=pd.concat([xcont_data,xcat_ohe],axis=1)\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(concat_cat_cont_df,ydata_lb):\n",
    "    \n",
    "    '''\n",
    "    The function is to train-test split the dataframe with transformed categorical data and continuous data.\n",
    "    \n",
    "    Argument :- \n",
    "    concat_cat_cont_df  :- Concatenated cat and cont data,\n",
    "    ydata_lb               :- LabelEncoded Y data.\n",
    "    \n",
    "    Return :- \n",
    "    x_train:-training data\n",
    "    x_test :- Validation data\n",
    "    y_train :- Label data of training data\n",
    "    y_test :- label data of validation data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    x_train,x_test,y_train,y_test=train_test_split(concat_cat_cont_df,ydata_lb,test_size=0.3,random_state=0)\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(xtrain,ydata):\n",
    "    '''\n",
    "    Create an object of Decision Tree and fit the training X and Y data.\n",
    "    \n",
    "    Argument :- \n",
    "    xtrain :- Training data\n",
    "    ydata  :- Y data of training.\n",
    "    \n",
    "    Return :- object of DT\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dt=DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    dt.fit(xtrain,ydata)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test(dt_obj,xtest):\n",
    "    \n",
    "    '''\n",
    "    The function is to run the test data over the model to get the prediction of every row.\n",
    "    \n",
    "    Argument :- \n",
    "    dt_obj :- Decision Tree object \n",
    "    xtest  :- test data.\n",
    "    \n",
    "    Return :- \n",
    "    ypred :- Array of predicted class for every row.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ypred=dt_obj.predict(xtest)\n",
    "    \n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(ytest,y_pred):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the accuracy score of predicted data and actual Y data of test data set.\n",
    "    \n",
    "    Argument :- \n",
    "    ytest :- Test data\n",
    "    y_pred :-Predicted arrray of class \n",
    "            \n",
    "    '''\n",
    "\n",
    "    print(metrics.accuracy_score(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_split(xtrain,ytrain):\n",
    "    \n",
    "    '''\n",
    "    The function is used to use cross validation with cross_val_score method.This method take parameters classifier object,\n",
    "    training data, Y data,cv=number of fold,scoring=accuracy. It returns an average score of the folds.\n",
    "    \n",
    "    Argument :- \n",
    "    xtrain :- Training data.\n",
    "    ytrain :- Y data.\n",
    "   \n",
    "    '''\n",
    "    \n",
    "    scores=cross_val_score(DecisionTreeClassifier(criterion=\"entropy\"),xtrain,ytrain,cv=4,scoring='accuracy')\n",
    "    dt_score=scores.mean()\n",
    "    print(dt_score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_dt():\n",
    "    \n",
    "    '''\n",
    "    The function is to run multiple classifiers using GridSearchCV and the find the best classifier.The library takes multiple\n",
    "    classifier objects. Create a param_grid dictionary with parameters for each of the classifiers and a range of values. \n",
    "    Create a Pipeline object listing classifier objects.Create a object of GridSearchCV with pipeline object,param_grid \n",
    "    dictionary and cv=3 (cross-validation).Fit the training data on this object. The attributes of GridSearchCV are printed to\n",
    "    give the best classifier with optimal parameters and values along with an accuracy score.\n",
    "    \n",
    "    '''\n",
    "    k_range = list(range(3,9,2))\n",
    "    m_depth=list(range(10,12))\n",
    "    m_split=list(range(30,35))\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "\n",
    "    param_grid = [{\n",
    "        'classifier':[KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors':k_range,\n",
    "        },\n",
    "        {\n",
    "        'classifier':[DecisionTreeClassifier()],\n",
    "        'classifier__max_depth': m_depth,\n",
    "        'classifier__min_samples_split':m_split,\n",
    "        'classifier__criterion':['entropy','gini']\n",
    "       }] \n",
    "\n",
    "\n",
    "    clf=GridSearchCV(pipe,param_grid,cv=3)\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    print(clf.best_estimator_)\n",
    "    print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_display(dt_obj,xtrain,oe_obj):\n",
    "    \n",
    "    '''\n",
    "    Use Graphviz to graphically display the Decsion Tree.It also allows to store the tree in pdf format.\n",
    "    \n",
    "    Argument :-\n",
    "    dt_obj  :- Decision tree object\n",
    "    xtrain  :- Training data\n",
    "    oe_obj \n",
    "    \n",
    "    '''\n",
    "    label=list(dt_obj.classes_.astype('str'))\n",
    "    \n",
    "    dot_data = tree.export_graphviz(dt_obj, out_file=None, feature_names=list(xtrain.columns), class_names=label,filled=True, rounded=True,special_characters=True)  \n",
    "      \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    graph.write_pdf(\"Adult_data_new.pdf\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data using pandas and convert in data frame\n",
    "df=read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into X and Y data.\n",
    "xdata,ydata=preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncode Y data for graphically displaying the Decision Tree.\n",
    "ydata_lb,le_obj=labelencode(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the X data into categorical and continuous data.\n",
    "xcat_data,xcont_data=cat_cont_data_split(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the categorical data into interger format using OneHotEncoder.\n",
    "xcat_ohe,oe_obj=onehotencode(xcat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate transformed categorical data with continuous data.\n",
    "concat_cat_cont_df=concat_cat_cont(xcat_ohe,xcont_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split the concatenated data.\n",
    "xtrain,xtest,ytrain,ytest=train_test(concat_cat_cont_df,ydata_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create object of Decision Tree\n",
    "dt_obj=decision_tree(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the test data over the model to predict the data.\n",
    "y_pred=pred_test(dt_obj,xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8172791483263384\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy score of predicted and actual Y data.\n",
    "accuracy_score(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157246665787077\n"
     ]
    }
   ],
   "source": [
    "#Run Decision Tree with cross-validation.\n",
    "cross_val_split(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=11,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=33,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "0.8549491049491049\n"
     ]
    }
   ],
   "source": [
    "#Run K-nearest and Decision Tree classifier with their respective multilple parameters and range of values using GridSearchCV.\n",
    "grid_search_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphically display Decision tree and save the image in pdf format.\n",
    "graph_display(dt_obj,xtrain,oe_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
